
//#define CMSIS_SVM 
//#define CUSTOM_SVM
//#define CUSTOM_NN
#define CMSIS_NN



//#include <stdio.h>
//#include "MK64F12.h"
//#include "arm_math.h"
//#include <math.h>

//CMSIS implementation of support vector machines
#ifdef CMSIS_SVM
#define MASK(X)   (1UL << (X))
#define BLUE_LED  22  // PTB21 and PTB22 for RED LED

/* 
  The polynomial SVM instance containing all parameters.
  Those parameters can be generated with the python library scikit-learn.
 */
arm_svm_polynomial_instance_f32 params;
/*
  Parameters generated by a training of the SVM classifier
  using scikit-learn and some random input data.
 */
#define NB_SUPPORT_VECTORS 10
/*
  Dimension of the vector space. A vector is your feature.
  It could, for instance, be the pixels of a picture or the FFT of a signal.
 */
#define VECTOR_DIMENSION 2
const float32_t dualCoefficients[NB_SUPPORT_VECTORS]={-0.03802639f, -0.00724999f, -0.34707007f, -0.08643168f,
  0.09098222f,  0.07258172f,
  0.21660862f,  0.02447408f,  0.02533858f,  0.04879292f}; /* Dual coefficients */
const float32_t supportVectors[NB_SUPPORT_VECTORS*VECTOR_DIMENSION]={-0.93798093f, -1.66140533f,
	-1.5027687f,   1.31667668f,
	-0.66792864f,  1.92006599f,
  1.431858f,   -1.19567697f,
  1.11978873f,  2.3759851f,
	1.86927278f, -1.92056283f,
 -1.38384115f,  2.25891395f,
	-0.48367566f, -2.64640221f,
  2.46149547f, -1.01100254f,
 -1.99213754f, -1.85171406f}; /* Support vectors */
/*
  Class A is identified with value 0.
  Class B is identified with value 1.
  
  This array is used by the SVM functions to do a conversion and ease the comparison
  with the Python code where different values could be used.
 */
const int32_t   classes[2]={0,1};
int32_t main(void)
{

//	  SIM->SCGC5 |= SIM_SCGC5_PORTB_MASK;

//    // Configure PTB21 as GPIO
//    PORTB->PCR[BLUE_LED] &= ~PORT_PCR_MUX_MASK;
//    PORTB->PCR[BLUE_LED] |= PORT_PCR_MUX(1);

//    // Set PTB21 as output
//    PTB->PDDR |= MASK(BLUE_LED);
  /* Array of input data */
  float32_t in[VECTOR_DIMENSION];
  /* Result of the classifier */
  int32_t result;
  
  /*
    Initialization of the SVM instance parameters.
    Additional parameters (intercept, degree, coef0 and gamma) are also coming from Python.
   */
  arm_svm_polynomial_init_f32(&params,
    NB_SUPPORT_VECTORS,
    VECTOR_DIMENSION,
    -3.300079f,        /* Intercept */
    dualCoefficients,
    supportVectors,
    classes,
    3,                 /* degree */
    1.100000f,         /* Coef0 */
    0.500000f          /* Gamma */
  );
  /*
    Input data.
    It is corresponding to a point inside the first class.
   */
 
  while (1){
		
		in[0] = 0.4f;
    in[1] = 0.1f;
	
		arm_svm_polynomial_predict_f32(&params, in, &result);
		
//		if(result==0) PTB->PCOR = MASK(BLUE_LED);
//		
//		else PTB->PSOR = MASK(BLUE_LED);
		
			
		
	
	}; /* main function does not return */

}
#endif 


//Custom implementation of support vector machines

/*---------------------------------------------------*/

#ifdef CUSTOM_SVM

#include <stdio.h>
#include <math.h>
#include <stdint.h>
#include "MK64F12.h"

#define MASK(X)   (1UL << (X))
#define BLUE_LED  21  // PTB21 and PTB22 for RED LED

#define NUM_SUPPORT_VECTORS 10
#define NUM_FEATURES 2

// Define the polynomial kernel function with gamma
float32_t polynomial_kernel(float32_t x[], float32_t y[], int32_t degree, float32_t coef0, float32_t gamma) {
    float32_t dot_product = 0.0;
    for (int32_t i = 0; i < NUM_FEATURES; i++) {
        dot_product += x[i] * y[i];
    }
    return pow(gamma * dot_product + coef0, degree);
}

// Compute the decision function
float32_t compute_decision_function( float32_t  support_vectors[NUM_SUPPORT_VECTORS][NUM_FEATURES],
                                 float32_t  dual_coeffs[NUM_SUPPORT_VECTORS],
                                 float32_t  intercept,
                                 float32_t  input[],
                                 int32_t degree,
                                 float32_t  coef0,
                                 float32_t  gamma) {
    float32_t decision_value = 0.0;
    for (int i = 0; i < NUM_SUPPORT_VECTORS; i++) {
        float32_t  kernel_value = polynomial_kernel(support_vectors[i], input, degree, coef0, gamma);
        decision_value += dual_coeffs[i] * kernel_value;
    }
    decision_value += intercept;
    return decision_value;
}

// Make prediction based on the decision function
int32_t predict(float32_t decision_value) {
    return decision_value >= 0 ? 1 : -1;
}

int main() {

// SIM->SCGC5 |= SIM_SCGC5_PORTB_MASK;

//    // Configure PTB21 as GPIO
//    PORTB->PCR[BLUE_LED] &= ~PORT_PCR_MUX_MASK;
//    PORTB->PCR[BLUE_LED] |= PORT_PCR_MUX(1);

//    // Set PTB21 as output
//    PTB->PDDR |= MASK(BLUE_LED);




    // Define the support vectors (obtaining from training the model in python)
   const float32_t supportVectors[NUM_SUPPORT_VECTORS][NUM_FEATURES]={-0.93798093f, -1.66140533f,
	-1.5027687f,   1.31667668f,
	-0.66792864f,  1.92006599f,
  1.431858f,   -1.19567697f,
  1.11978873f,  2.3759851f,
	1.86927278f, -1.92056283f,
 -1.38384115f,  2.25891395f,
	-0.48367566f, -2.64640221f,
  2.46149547f, -1.01100254f,
 -1.99213754f, -1.85171406f};

    // Define the dual coefficients (from python)
  const float32_t dualCoefficients[NUM_SUPPORT_VECTORS]={-0.03802639f, -0.00724999f, -0.34707007f, -0.08643168f,
  0.09098222f,  0.07258172f,
  0.21660862f,  0.02447408f,  0.02533858f,  0.04879292f};

    // Define the intercept (example value)
    float32_t intercept = -3.300079;

    // Define the degree of the polynomial kernel, coef0, and gamma
    int32_t degree = 3;
    float32_t coef0 = 1.10000;
    float32_t gamma = 0.5;
	
		
		while(1){
			    // Define the input feature vector for prediction
    float32_t input[NUM_FEATURES] = {0.4, 0.1};
			 // Compute the decision function value
			
			 float32_t decision_value = compute_decision_function(supportVectors, dualCoefficients, intercept, input, degree, coef0, gamma);
		 // Make the prediction
			int32_t predicted_class = predict(decision_value);
			
//			if (predicted_class==-1) PTB->PCOR = MASK(BLUE_LED);
//			
//			else PTB->PSOR = MASK(BLUE_LED);
//		
		
		}

    return 0;
}

#endif 




//custom implementation of artificial neural networks

#ifdef CUSTOM_NN


#include <stdio.h>
#include <stdint.h>
#include <math.h>

#define MASK(X)   (1UL << (X))
#define BLUE_LED  21  // PTB21 and PTB22 for RED LED

#define INPUT_WEIGHT_ROW (18)
#define INPUT_WEIGHT_COL (5)

#define L1_BIAS_ROW (18)
#define L2_BIAS_ROW (18)

#define L1_WEIGHT_ROW (18)
#define L1_WEIGHT_COL (18)

#define OUTPUT_WEIGHT_ROW (1)
#define OUTPUT_WEIGHT_COL (18)

//storing the weights and biases of each stage 

float inputWeights[INPUT_WEIGHT_ROW][INPUT_WEIGHT_COL]={
    
    -0.7571,    0.6660,   -0.5187,   -0.3797,   -1.0735,
   -0.9644,   -0.7848,    0.7388 ,  -0.8827,    0.5754,
    0.9130 ,   1.0088,    0.3637,    0.7493,    0.8000,
   -0.3988,    0.0827,   -0.0666,   -0.6088,    0.0655,
   -0.3585,    0.4984,    0.7774,   -0.8738,   -0.8142,
   -0.1826,    1.0395,   -0.5762,   -0.3447,    0.7963,
   -0.0722,  -0.4324,    0.4567,   -0.0411,   -0.2712,
    0.1594,   -0.1909 ,   0.4579,   -0.8320,   -0.4823,
   -0.9496,   -0.0703,    0.1237,    0.9908,    0.4926,
    0.4114,    0.4249,   -0.6317,   -0.3736,   -1.1597,
    0.0740,    0.6824,    0.2667,   -0.3718,   -0.9472,
    0.2609,   -0.8682,   -0.4527,   -0.8861,    0.0656,
   -0.4317,   -0.6106,   -0.7104,   -0.4483,    0.1267,
   -0.1839,   -0.2753,   -0.5267,   -0.6585,    0.1945,
   -0.7744,   -0.8403,    0.8567,  -0.0827,    0.4112,
   -0.6458,    0.0438,   -0.8571,    0.5229,    0.4145,
    0.3256,   -0.3283,   -0.5150,    0.2621,    0.5628,
   -0.3951,   -0.5201,   -0.8085,   -1.1402,   -0.6990

};


float layer1Weights[L1_WEIGHT_ROW][L1_WEIGHT_COL]= {
    0.6338,   -0.2552,   -0.4123,   -0.6251,   -0.7545,    0.7886,   -0.1840,   -0.1123,    0.4874,    0.6293,   -0.0851,    0.9066,    0.4772,     
    0.8043,   -0.1994,   -0.5247,   -0.4675,   -1.0925,    0.3847,    0.9848,   -0.3503,   -0.7882,    0.4303,   -0.5822,   -0.0942,    0.1715,
    0.3179,    0.2851,    0.0617,    0.5957,   -0.2938,    0.1104,   -0.2440,    0.0149,    0.3631,   -0.7605,    0.4534,   -0.8974,   -0.6972,
   -0.5985,    0.2669,   -0.8377,   -0.0248,    0.2707,   -0.6049,    0.2487,    0.2912,   -0.0735,    0.1133,   -0.4964,    0.5591,    0.7759,
    0.4495,    0.5776,   -0.1894,    0.5379,    0.4420,   -0.5759,   -0.6908,   -0.1519,   -0.5757,   -0.3482,   -0.8746,    0.2654,   -0.4613,
   -0.8404,   -0.3031,   -0.7903,   -0.2082,   -0.0015,   -0.8453,   -0.2700,   -0.2251,   -0.8030,    0.0874,    0.5223,   -0.3659,    0.7259,
   -0.7649,   -0.5879,   -0.7752,   -0.4541,   -0.7822,    0.8280,   -0.6777,   -0.7499,    0.6471,   -0.2022,    0.3425,    0.9945,   -0.5521,
    0.3363,   -0.8156,    0.5789,  -0.9255,    0.4494,    0.3971,    0.5571,   -0.7891,   -0.6500,   -0.1638,    0.5169,   -0.5341,   -0.2279,
   -0.3424,    0.5439,   -0.4169,    0.3466,   -0.7470,    0.1156,    0.7422,   -0.4196,   -0.6729,   -0.6385,    0.2841,    0.3049,   -0.8250,
    0.3076,   -0.5887,    0.2071,   -0.1409,   -0.7314,   -0.3731,   -0.2984,   -0.3650,    0.3320,   -0.4892,   -0.1619,    0.2100,    0.2802,
    0.4983,   -0.2235,    0.9288,   -0.0965,   -0.8028,   -0.6676,    0.3711,    0.3074,    0.7888,   -0.9589,   -0.2185,   -0.2255,   -0.6388,
    0.1593,    0.1036,   -0.1375,    0.2197,   -0.7275,    0.2428,   -0.4120,    0.9045,    0.0331,    0.8442,    0.6251,   -0.7164,   -0.9117,
    0.3853,   -0.5693,    0.4056,   -0.8810,   -0.5907,    0.9554,    0.0568,    1.0061,    0.4054,    0.2502,   -0.3710,   -0.8733,    0.4835,
   -0.6534,    0.2218,    0.5085,   -0.3684,   -0.8226,   -0.6703,    0.6195,   -0.2990,   -0.6928,    0.8209,    0.5110,   -0.3011,   -0.4780,
    0.4877,   -0.0291,   -0.1347,    0.5454,   -0.3491,   -0.4844,    0.1972,   -0.5097,    0.9069,   -0.6702,    0.5862,   -0.6310,    0.3308,
    1.0769,   -0.5378,    0.2859,    0.3928,   -0.2527,   -0.1683,   -0.2607,    0.4771,    0.0818,    0.9426,    0.8294,    0.3490,   -0.3676,
    0.8531,    0.6377,   -0.7836,   -0.7494,   -0.4184,   -0.8298,   -0.3610,    0.6111,    0.3595,    0.6514,    0.1176,   -0.2115,    0.2799,
   -0.8320,   -0.7991,    0.8609,   -0.7397,   -0.5110,    0.3618,   -0.0950,   0.4670,   -0.9269,    0.1510,    0.2625,    0.6741,   -0.9622,

    0.8358,    0.9776,   -0.1787,    0.0236,    0.8575,
    0.5864,   -0.8069,    0.9687,   -0.8348,    0.5766,
    0.4412,    0.8108,    0.8912,    0.4391,   -0.9159,
    0.5068,   -0.9807,    0.3533,    0.9923,   -0.0045,
   -0.2298,    0.3692,    0.9766,   -0.2909,    0.4142,
    0.1489,    0.5237,    0.5337,    0.9425,    0.3959,
    0.1515,    0.0683,   -0.3266,   -0.3071,   -0.5514,
    0.0966,    0.7816,    0.3248,    0.7731,   -0.5145,
   -0.4499,    0.7980,   -0.5117,   -0.0906,    0.3461,
   -0.5027,    0.2519,   -0.4090,   -0.1731,   -0.0450,
   -0.0967,   -0.7243,    0.3604,   -0.5645,    0.2474,
   -0.5484,   -0.5644,    0.0557,   -0.7487,   -0.5271,
    0.6600,   -0.6287,   -0.1768,   -0.3822,   -0.8347,
    0.7668,   -0.9397,    0.2053,    0.4522,    0.6018,
   -0.9328,   -0.7847,    0.5010,    0.5657,    0.5388,
   -0.0114,    0.2508,    0.1671,    0.3876,    1.1429,
   -0.7724,    0.8695,    0.1036,   -0.9804,   -0.6176,
    0.5932,   -0.2911,    0.1671,    0.6864,   -0.6356

};


float outputWeights[OUTPUT_WEIGHT_ROW][OUTPUT_WEIGHT_COL]={ -0.0117, -0.4122, 2.1841, 0.7022, -0.0373,1.7327,-1.6016,-0.6091,1.8883,1.8447, 0.9437, 0.5219, -0.8824, 1.8595, -1.6584, 1.4977, 0.5731, 1.4688};



float layer1Biases[L1_BIAS_ROW]={
    
    0.4257,
   -0.0073,
   -0.1582,
   -0.8742,
    0.6628,
   -0.3277,
    0.3286,
    0.3261,
   -0.7903,
   -0.8343,
   -0.1007,
   -0.3098,
    0.6787,
    0.3649,
    0.4860,
   -0.8973,
   -0.8542,
   -0.7550
};


double layer2Biases[L2_BIAS_ROW]={
    
   -0.7874,
   -0.0544,
   -0.6742,
    0.5088,
   -0.7994,
   -0.9795,
    0.1149,
    0.6581,
   -0.3761,
   -0.6420,
   -0.3221,
   -0.5864,
    0.1938,
    0.6602,
    0.2666,
   -1.0642,
   -0.2652,
   -0.9017
};

 float outputBiases[1]={-5.6766};

 float L1_OUTPUT[L1_WEIGHT_ROW]; //this is the output after hidden layer 1 (8X1 Matrix)
 
 float L2_OUTPUT[L2_BIAS_ROW]; //output after hidden layer 2 (8x1 Matrix)
 
 float FINAL_OUTPUT[1]; // final output to be fed to sigmoid activation function (1X1 Matrix)
 
 
 //defining activation function: Relu and Sigmoid 
 
 //ReLu max(value, 0)
 float relu( float value){
     
     return value>0? value:0; 
 }

//sigmoid (1/(1+exp(-value)))

float sigmoid (float value){
    
    return 1/(1+exp(-value));
}


//matrix multiplication of (input* inputWeights) + layer1biases

void Layer1(float *INPUT, float *L1_OUTPUT){
    float temp; 
    for(int i=0; i<INPUT_WEIGHT_ROW; i++){
        temp=0; 
        for (int j=0; j<INPUT_WEIGHT_COL; j++){
            temp+= (INPUT[j]*inputWeights[i][j]);
        }
        
        temp= temp+ layer1Biases[i];
        L1_OUTPUT[i]= relu(temp);
    }
    
}


//matrix multiplication of layer2 (layer1Weights*L1_OUPUT + layer2Biases)

void Layer2 (float *L1_OUTPUT, float *L2_OUTPUT){
    float temp;
    for (int i=0; i<L1_WEIGHT_ROW; i++){
        temp=0; 
        for (int j=0; j<L1_WEIGHT_COL; j++){
            
            temp+= (L1_OUTPUT[j]*layer1Weights[i][j]);
        }
        
        temp= temp+ layer2Biases[i];
        L2_OUTPUT[i]= relu(temp);
    }
    
}

// output layer that will be activated by sigmoid 

void outputLayer(float *L2_OUTPUT, float *FINAL_OUTPUT){
    float temp; 
    for(int i=0; i<OUTPUT_WEIGHT_ROW; i++){
        temp=0; 
        for(int j=0; j<OUTPUT_WEIGHT_COL; j++){
            temp+= (L2_OUTPUT[j]*outputWeights[i][j]);
        }
        
        temp= temp+ outputBiases[i];
        FINAL_OUTPUT[i]= sigmoid(temp);
    }
}

//----------beginning of main() program---------------------------

int main(){
	
//		SIM->SCGC5 |= SIM_SCGC5_PORTB_MASK;

//    // Configure PTB21 as GPIO
//    PORTB->PCR[BLUE_LED] &= ~PORT_PCR_MUX_MASK;
//    PORTB->PCR[BLUE_LED] |= PORT_PCR_MUX(1);

//    // Set PTB21 as output
//    PTB->PDDR |= MASK(BLUE_LED);
	
    
		
		while(1){
		float INPUT[5]={0.1, 2, 5, 2, 0}; 
    
    Layer1(INPUT, L1_OUTPUT); 
    Layer2(L1_OUTPUT, L2_OUTPUT);
    outputLayer(L2_OUTPUT, FINAL_OUTPUT);
    
        
        //printf("the number obtained is: %f\n", FINAL_OUTPUT[0]);	
//			
//		if (FINAL_OUTPUT[0]>0.5) PTB->PCOR = MASK(BLUE_LED);
//		else PTB->PSOR = MASK(BLUE_LED);
			
		}
 
    return 0;
}


#endif 



//CMSIS implementation of artificial neural networks

#ifdef CMSIS_NN


#include <stdio.h>
#include "arm_nnfunctions.h"
#include "arm_nnsupportfunctions.h"




int8_t inputWeightsQ[18][5]={
    
    -81 ,   84,   -53,   -37,  -117,
  -105,   -84,    93,   -95,    74,
   113,   124,    49,    94,   100,
   -39,    17,    -1,   -64,    15,
   -35,    65,    97,   -94,   -87,
   -14,   127,   -60,   -33,    99,
    -1,   -43,    60,     2,   -24,
    25,   -15,    60,   -89,  -49,
  -103,    -1,    21,   122,    64,
    55,    56,   -66,   -36,  -127,
    16,    86,    38,   -36,  -103,
    37,   -94,   -45,   -96,    15,
   -43,   -64,   -75,  -45,    22,
   -14,   -25,   -54,   -69,    30,
   -83,   -90,   106,    -3,    55,
   -68,    12,   -92,    68,    55,
    45,   -31,   -53,    37,    72,
   -39,   -53,   -87,  -125,   -74
    
};  //qmin=128 qmax= 127 xmin=-1.1597 xmax= 1.0395 S=0.0086 Z=7 (quantization parameters)


int32_t layer1BiasesQ[18]={
    
    56,
     6,
   -11,
   -95,
    84,
   -31,
    45,
    45,
   -85,
   -90,
    -5,
   -29,
    86,
    49,
    64,
   -97,
   -92,
   -81
    
}; //same quantization parameters as inputWeightsQ


int8_t layer1WeightsQ[18][18]={
    
    68,  -33,   -51,   -75,   -90,    86,   -25,   -17,    52,    68,   -14,    99,    50,    91,   108,   -24,    -1,    94,
    88, -27,   -64,   -57,  -128,    40,   108,   -44,   -94,    45,   -70,   -15,    16,    63,   -96,   107,   -99,    62,
    32,  29,     3,    64,   -38,     9,   -32,    -2,    37,   -91,    48,  -106,   -84,    46,    88,    98,    46,  -108,
   -72, 26,  -100,    -7,    27,   -73,    24,    29,   -12,     9,   -61,    60,    85,    54,  -116,    36,   109,    -5,
    47, 62,   -26,    57,    46,   -70,   -83,   -21,   -70,   -44,  -104,    26,   -57,   -30,    38,   107,   -37,    43,
  -100, -39,   -94,   -28,    -4,  -100,   -35,   -30,   -96,     6,    56,   -46,    79,    13,    56,    57,   104,    41,
   -91, -71,  -92,   -56,   -93,    90,   -81,   -90,    70,   -27,    35,   109,   -67,    13,     4,   -41,   -39,   -67,
    34, -97,    62,  -110,    47,    41,    60,   -94,   -78,   -23,    55,   -65,   -30,     7,    85,    33,    84,   -63,
   -43, 58,   -52,    36,  -89,    9,    81,   -52,   -81,   -77,    28,    31,   -98,   -55,    87,   -62,   -14,    35,
    31, -71,    20,   -20,   -87,   -47,   -38,   -46,    34,   -60,   -22,    20,    28,   -61,    25,   -51,   -24,   -9,
    53, -29,   102,   -15,   -96,   -80,   38,   31,    86,  -113,   -29,   -30,   -77,   -15,   -87,    37,   -68,    24,
    14,   8,   -20,    21,   -87,    24,   -51,    99,     0,    92,    67,   -86,  -108,   -67,   -68,     2,   -89,   -64,
    40,   -69,    42,  -105,   -71,   105,     2,   111,    42,    25,   -46,  -104,    51,    71,   -76,   -24,   -48,   -99,
   -79,    21,    54,   -46,   -98,   -80,    67,   -38,  -83,    90,    54,   -38,   -59,    83,  -111,    19,    48,    65,
    52,    -7,   -19,    58,   -44,   -59,    18,   -62,    99,   -80,    63,   -76,    34,  -110,   -94,    53,    61,    57,
   119,   -65,    29,    41,   -33,   -23,   -34,    50,     5,   104,    91,    36,   -46,    -5,    25,    15,    40,  126,
    93,    69,   -93,   -89,   -52,   -99,   -45,    66,    37,    70,     9,   -28,    28,   -92,    95,     8,  -116,   -74,
   -99,   -95,    94,   -88,   -62,    37,   -15,    49, -110,    13,    26,    73,  -114,    64,   -37,    15,    74,   -77
};


int32_t layer2BiasesQ[18][1]={
    
   -94,
   -10,
   -81,
    54,
   -95,
  -116,
     9,
    71,
   -47,
   -77,
   -41,
   -71,
    18,
    71,
    26,
  -125,
   -34,
  -107
};


int8_t outputWeightsQ[1][18]= {
    
    -19, -45, 127, 29,-20,97,-124,-58,107,104,45,17,-77,105,-128,81,20,79
};

int32_t outputBiasQ[1]={-128};


int8_t pOut1[18][1];
int8_t pOut2[18][1];
int8_t pOut3[1];
int16_t finalOutput[1];


int main(){

//parameters for operation between input vector and weigth1 matrix
	
	cmsis_nn_fc_params fc_params={7,7,7,0,127};
	cmsis_nn_context ctx={NULL,0};
	cmsis_nn_per_tensor_quant_params quant_params={1,0};
	cmsis_nn_dims input_dims={1,5,1,1};
	cmsis_nn_dims weight_dims= {1,18,5,1};
	cmsis_nn_dims bias_dims={1,18,1,1};
	cmsis_nn_dims output_dims={1,18,1,1};
	
	//parameters for operation between hiddenlayer1 and hiddenlayer 2
	
	cmsis_nn_fc_params fc_params1={-17,-17,-17,0,127}; //by clipping [0,127] we have used ReLu
	cmsis_nn_dims input2_dims={1,18,1,1};
	cmsis_nn_dims weight2_dims= {1,18,18,1};
	cmsis_nn_dims bias2_dims={1,18,1,1};
	cmsis_nn_dims output2_dims={1,18,1,1};
	
	//parameters for operation between hiddenlayer2 and outputlayer 
	
	cmsis_nn_fc_params fc_params2={-18,-18,-18,-128,127}; //[-128,127] No ReLu applied
	cmsis_nn_dims input3_dims={1,18,1,1};
	cmsis_nn_dims weight3_dims= {1,1,18,1};
	cmsis_nn_dims bias3_dims={1,1,1,1};
	cmsis_nn_dims output3_dims={1,1,1,1};
	
	
	
	while(1){
	
		int8_t INPUT[5]={12, 13, -4,6,7};
		
		//computation between input vector and hiddenlayer1 
		
		arm_fully_connected_s8(&ctx,&fc_params,&quant_params,&input_dims,INPUT,&weight_dims,*inputWeightsQ,&bias_dims,           
        layer1BiasesQ,&output_dims,*pOut1);
		
		//computation between hiddenlayer1 and hiddenlayer2 
		arm_fully_connected_s8(&ctx,&fc_params1,&quant_params,&input2_dims,*pOut1,&weight2_dims,*layer1WeightsQ,&bias2_dims,           
        *layer2BiasesQ,&output2_dims,*pOut2);
		
		//computation between hiddenlayer2 and outputlayer
		
		arm_fully_connected_s8(&ctx,&fc_params2,&quant_params,&input3_dims,*pOut2,&weight2_dims,*outputWeightsQ,&bias3_dims,           
        outputBiasQ,&output3_dims,pOut3);
		
		//the ouput pOut3 needs to go through the sigmoid activation function for final ouput 
		
		arm_nn_activation_s16((int16_t *)pOut3, finalOutput,1,0,ARM_SIGMOID);
	
	
	}


		return 0; 
}


#endif 